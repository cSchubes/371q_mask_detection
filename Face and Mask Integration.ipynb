{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face and Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import helpers\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import glob\n",
    "import random\n",
    "from retinaface import RetinaFace\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Facial Recognition\n",
    "We use retinaface to detect facial features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model[normal quality] init ..\n",
      "model success !\n"
     ]
    }
   ],
   "source": [
    "# Initialize facial detection\n",
    "detector = RetinaFace(quality = \"normal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The actual detection of faces\n",
    "img = detector.read(\"test.jpg\")\n",
    "faces = detector.predict(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crop images\n",
    "img = Image.open(\"test.jpg\")\n",
    "cropImages = []\n",
    "for face in faces:\n",
    "    cropImages.append(img.crop((face['x1'], face['y1'], face['x2'], face['y2'\n",
    "    ])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the faces and original image\n",
    "#%matplotlib inline\n",
    "#plt.imshow(np.asarray(img))\n",
    "#plt.figure()\n",
    "#for i in cropImages:\n",
    "#    plt.figure()\n",
    "#    plt.imshow(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mask Detection\n",
    "we will now pass cropImages through the mask classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = 64\n",
    "\n",
    "# I am passing in PIL images, this converts it to PIL\n",
    "def resize_img(pic):\n",
    "    image_array = tf.keras.preprocessing.image.img_to_array(pic)\n",
    "    img = tf.convert_to_tensor(image_array, dtype = 'uint8')\n",
    "    img = tf.image.resize(img, [dim, dim])\n",
    "    img = img/127.5-1\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The neural network\n",
    "\n",
    "def conv_model(num_blocks, rate, dim):\n",
    "    \n",
    "    input = tf.keras.layers.Input(shape=(dim,dim,3))\n",
    "    \n",
    "    x = tf.keras.layers.Conv2D(16, (16,16), strides=1)(input)\n",
    "    x = tf.keras.layers.Dropout(rate)(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Activation('relu')(x)\n",
    "        \n",
    "    for i in range(num_blocks-1):\n",
    "        x = tf.keras.layers.Conv2D(8, (8,8), strides=1)(x)\n",
    "        x = tf.keras.layers.Dropout(rate)(x)\n",
    "        x = tf.keras.layers.BatchNormalization()(x)\n",
    "        x = tf.keras.layers.Activation('relu')(x)\n",
    "        \n",
    "    x = tf.keras.layers.Conv2D(filters=2, kernel_size=1, strides=1)(x)\n",
    "    x = tf.keras.layers.Dropout(rate)(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.GlobalMaxPooling2D()(x)\n",
    "    predictions = tf.keras.layers.Activation('softmax')(x)\n",
    "    \n",
    "    model = tf.keras.Model(inputs=input, outputs=predictions)\n",
    "    print(model.summary())\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 64, 64, 3)]       0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 49, 49, 16)        12304     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 49, 49, 16)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 49, 49, 16)        64        \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 49, 49, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 42, 42, 8)         8200      \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 42, 42, 8)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 42, 42, 8)         32        \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 42, 42, 8)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 35, 35, 8)         4104      \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 35, 35, 8)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 35, 35, 8)         32        \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 35, 35, 8)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 35, 35, 2)         18        \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 35, 35, 2)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 35, 35, 2)         8         \n",
      "_________________________________________________________________\n",
      "global_max_pooling2d_1 (Glob (None, 2)                 0         \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 24,762\n",
      "Trainable params: 24,694\n",
      "Non-trainable params: 68\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "num_blocks = 3\n",
    "dropout = .1\n",
    "model = conv_model(num_blocks,.1, dim)\n",
    "model.load_weights('mask_classification_model_3_50.h5')\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction function, passes in PIL image\n",
    "def pred(pic):\n",
    "    im = resize_img(pic) # Convert and resize PIL image to 64x64 tensor\n",
    "    vals = model.predict(np.expand_dims(im,axis=0))\n",
    "    print(f\"Mask Val: {vals[0][0]}, Non-mask Val: {vals[0][1]}\")\n",
    "    prediction = np.argmax(vals,axis=1)\n",
    "    print(f\"Image predicted as {prediction}\")\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mask Val: 0.9910832643508911, Non-mask Val: 0.008916682563722134\n",
      "Image predicted as [0]\n",
      "Mask Val: 0.02029706910252571, Non-mask Val: 0.979702889919281\n",
      "Image predicted as [1]\n",
      "Mask Val: 0.9475284814834595, Non-mask Val: 0.05247148498892784\n",
      "Image predicted as [0]\n",
      "Mask Val: 0.9781445860862732, Non-mask Val: 0.021855473518371582\n",
      "Image predicted as [0]\n",
      "Mask Val: 0.9916129112243652, Non-mask Val: 0.00838708970695734\n",
      "Image predicted as [0]\n",
      "Mask Val: 0.9508954882621765, Non-mask Val: 0.04910454526543617\n",
      "Image predicted as [0]\n",
      "Mask Val: 0.9912887811660767, Non-mask Val: 0.008711186237633228\n",
      "Image predicted as [0]\n",
      "Mask Val: 0.9964062571525574, Non-mask Val: 0.0035937544889748096\n",
      "Image predicted as [0]\n",
      "Mask Val: 0.9973682761192322, Non-mask Val: 0.002631762996315956\n",
      "Image predicted as [0]\n",
      "Mask Val: 0.9831631779670715, Non-mask Val: 0.016836853697896004\n",
      "Image predicted as [0]\n",
      "Mask Val: 0.9968957901000977, Non-mask Val: 0.003104238538071513\n",
      "Image predicted as [0]\n",
      "Mask Val: 0.9966210126876831, Non-mask Val: 0.0033790171146392822\n",
      "Image predicted as [0]\n",
      "Mask Val: 0.9819011092185974, Non-mask Val: 0.0180988609790802\n",
      "Image predicted as [0]\n"
     ]
    }
   ],
   "source": [
    "maskResults = []\n",
    "for face in cropImages:\n",
    "    result = pred(face)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
