{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face and Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import helpers\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import glob\n",
    "import random\n",
    "from retinaface import RetinaFace\n",
    "from PIL import Image\n",
    "from od_metrics.BoundingBox import BoundingBox\n",
    "from od_metrics.BoundingBoxes import BoundingBoxes\n",
    "from od_metrics.utils import BBFormat, BBType\n",
    "from od_metrics.Evaluator import Evaluator\n",
    "use_pickle = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Kaggle Dataset\n",
    "Kaggle dataset has 853 images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load images and associated faces\n",
    "# this takes a second since we load with cv2\n",
    "imgs_with_labels = helpers.load_kaggle_863('../kaggle_dataset_863')\n",
    "imgs_with_labels = helpers.convert_kaggle_863_for_metrics(imgs_with_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Facial Recognition\n",
    "We use retinaface to detect facial features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model[normal quality] init ..\n",
      "model success !\n"
     ]
    }
   ],
   "source": [
    "# Initialize facial detection\n",
    "detector = RetinaFace(quality = \"normal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_detection_results = {}\n",
    "dl_detection_results['retinaface_mobilenet_alt'] = imgs_with_labels['bboxes'].clone()\n",
    "detections = dl_detection_results['retinaface_mobilenet_alt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n",
      "50\n",
      "75\n",
      "100\n",
      "125\n",
      "150\n",
      "175\n",
      "200\n",
      "225\n",
      "250\n",
      "275\n",
      "300\n",
      "325\n",
      "350\n",
      "375\n",
      "400\n",
      "425\n",
      "450\n",
      "475\n",
      "500\n",
      "525\n",
      "550\n",
      "575\n",
      "600\n",
      "625\n",
      "650\n",
      "675\n",
      "700\n",
      "725\n",
      "750\n",
      "775\n",
      "800\n",
      "825\n",
      "850\n"
     ]
    }
   ],
   "source": [
    "#Detect faces and crop them, add the cropped Faces into cropImages\n",
    "\n",
    "cropImages = []\n",
    "cnt = 0\n",
    "for img_name, img_data in imgs_with_labels['raw_data'].items():\n",
    "    img = img_data['img']\n",
    "    rgb_img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # using default threshold\n",
    "    faces = detector.predict(rgb_img.astype(np.float32), threshold=0.7)\n",
    "    \n",
    "    #Crop faces and add them to cropImages\n",
    "    img = Image.fromarray(rgb_img)\n",
    "    cropFaces = []\n",
    "    for face in faces:\n",
    "        cropFaces.append(img.crop((face['x1'], face['y1'], face['x2'], face['y2'])))\n",
    "        \n",
    "    cropImages.append(cropFaces)\n",
    "    \n",
    "    cnt += 1\n",
    "    if cnt%25 == 0: print(cnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mask Detection\n",
    "we will now pass cropImages through the mask classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = 64\n",
    "\n",
    "# I am passing in PIL images, this converts it to PIL\n",
    "def resize_img(pic):\n",
    "    image_array = tf.keras.preprocessing.image.img_to_array(pic)\n",
    "    img = tf.convert_to_tensor(image_array, dtype = 'uint8')\n",
    "    img = tf.image.resize(img, [dim, dim])\n",
    "    img = img/127.5-1\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The neural network\n",
    "\n",
    "def conv_model(num_blocks, rate, dim):\n",
    "    \n",
    "    input = tf.keras.layers.Input(shape=(dim,dim,3))\n",
    "    \n",
    "    x = tf.keras.layers.Conv2D(16, (16,16), strides=1)(input)\n",
    "    x = tf.keras.layers.Dropout(rate)(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Activation('relu')(x)\n",
    "        \n",
    "    for i in range(num_blocks-1):\n",
    "        x = tf.keras.layers.Conv2D(8, (8,8), strides=1)(x)\n",
    "        x = tf.keras.layers.Dropout(rate)(x)\n",
    "        x = tf.keras.layers.BatchNormalization()(x)\n",
    "        x = tf.keras.layers.Activation('relu')(x)\n",
    "        \n",
    "    x = tf.keras.layers.Conv2D(filters=2, kernel_size=1, strides=1)(x)\n",
    "    x = tf.keras.layers.Dropout(rate)(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.GlobalMaxPooling2D()(x)\n",
    "    predictions = tf.keras.layers.Activation('softmax')(x)\n",
    "    \n",
    "    model = tf.keras.Model(inputs=input, outputs=predictions)\n",
    "    print(model.summary())\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 64, 64, 3)]       0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 49, 49, 16)        12304     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 49, 49, 16)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 49, 49, 16)        64        \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 49, 49, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 42, 42, 8)         8200      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 42, 42, 8)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 42, 42, 8)         32        \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 42, 42, 8)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 35, 35, 8)         4104      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 35, 35, 8)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 35, 35, 8)         32        \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 35, 35, 8)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 35, 35, 2)         18        \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 35, 35, 2)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 35, 35, 2)         8         \n",
      "_________________________________________________________________\n",
      "global_max_pooling2d (Global (None, 2)                 0         \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 24,762\n",
      "Trainable params: 24,694\n",
      "Non-trainable params: 68\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "num_blocks = 3\n",
    "dropout = .1\n",
    "model = conv_model(num_blocks,.1, dim)\n",
    "model.load_weights('mask_classification_model_3_50.h5')\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction function, passes in PIL image\n",
    "def pred(pic):\n",
    "    im = resize_img(pic) # Convert and resize PIL image to 64x64 tensor\n",
    "    vals = model.predict(np.expand_dims(im,axis=0))\n",
    "    print(f\"Mask Val: {vals[0][0]}, Non-mask Val: {vals[0][1]}\")\n",
    "    prediction = np.argmax(vals,axis=1)\n",
    "    print(f\"Image predicted as {prediction}\")\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
