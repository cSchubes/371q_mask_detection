{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face and Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import helpers\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import glob\n",
    "import random\n",
    "from retinaface import RetinaFace\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Facial Recognition\n",
    "We use retinaface to detect facial features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model[normal quality] init ..\n",
      "model success !\n"
     ]
    }
   ],
   "source": [
    "# Initialize facial detection\n",
    "detector = RetinaFace(quality = \"normal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The actual detection of faces\n",
    "img = detector.read(\"test.jpg\")\n",
    "faces = detector.predict(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cropping the faces and saving them\n",
    "img = Image.open(\"test.jpg\")\n",
    "cropImages = []\n",
    "for face in faces:\n",
    "    cropImages.append(img.crop((face['x1'], face['y1'], face['x2'], face['y2'\n",
    "    ])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the faces and original image\n",
    "#%matplotlib inline\n",
    "#plt.imshow(np.asarray(img))\n",
    "#plt.figure()\n",
    "#for i in cropImages:\n",
    "#    plt.figure()\n",
    "#    plt.imshow(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mask Detection\n",
    "we will now pass cropImages through the mask classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = 64\n",
    "\n",
    "def read_img(file):\n",
    "    img = tf.io.read_file(file)\n",
    "    img = tf.image.decode_image(img, channels=3)\n",
    "    img = tf.image.resize(img, [dim, dim])\n",
    "    img = img/127.5-1\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_model(num_blocks, rate, dim):\n",
    "    \n",
    "    input = tf.keras.layers.Input(shape=(dim,dim,3))\n",
    "    \n",
    "    x = tf.keras.layers.Conv2D(16, (16,16), strides=1)(input)\n",
    "    x = tf.keras.layers.Dropout(rate)(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Activation('relu')(x)\n",
    "        \n",
    "    for i in range(num_blocks-1):\n",
    "        x = tf.keras.layers.Conv2D(8, (8,8), strides=1)(x)\n",
    "        x = tf.keras.layers.Dropout(rate)(x)\n",
    "        x = tf.keras.layers.BatchNormalization()(x)\n",
    "        x = tf.keras.layers.Activation('relu')(x)\n",
    "        \n",
    "    x = tf.keras.layers.Conv2D(filters=2, kernel_size=1, strides=1)(x)\n",
    "    x = tf.keras.layers.Dropout(rate)(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.GlobalMaxPooling2D()(x)\n",
    "    predictions = tf.keras.layers.Activation('softmax')(x)\n",
    "    \n",
    "    model = tf.keras.Model(inputs=input, outputs=predictions)\n",
    "    print(model.summary())\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 64, 64, 3)]       0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 49, 49, 16)        12304     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 49, 49, 16)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 49, 49, 16)        64        \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 49, 49, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 42, 42, 8)         8200      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 42, 42, 8)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 42, 42, 8)         32        \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 42, 42, 8)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 35, 35, 8)         4104      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 35, 35, 8)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 35, 35, 8)         32        \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 35, 35, 8)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 35, 35, 2)         18        \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 35, 35, 2)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 35, 35, 2)         8         \n",
      "_________________________________________________________________\n",
      "global_max_pooling2d (Global (None, 2)                 0         \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 24,762\n",
      "Trainable params: 24,694\n",
      "Non-trainable params: 68\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "num_blocks = 3\n",
    "dropout = .1\n",
    "model = conv_model(num_blocks,.1, dim)\n",
    "model.load_weights('mask_classification_model_3_50.h5')\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred(img_path):\n",
    "    im = read_img(img_path)\n",
    "    vals = model.predict(np.expand_dims(im,axis=0))\n",
    "    print(f\"Mask Val: {vals[0][0]}, Non-mask Val: {vals[0][1]}\")\n",
    "    pred = np.argmax(vals,axis=1)\n",
    "    print(f\"Image predicted as {pred}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
